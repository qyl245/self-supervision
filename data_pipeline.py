import os
import torch
from torch.utils.data import DataLoader
from collections import Counter, defaultdict

from datasets import KneePAD, MovePort
from sliding_window import SlidingWindowDataset
from logging_utils import setup_logger
from augmentations import create_ssl_transforms

logger = setup_logger()


class TransformSubsetDataset(torch.utils.data.Dataset):
    """
    ç”¨äºå¯¹æ•°æ®é›†å­é›†åº”ç”¨ transform,å¹¶é™„åŠ åŸŸæ ‡ç­¾
    """

    def __init__(self, dataset, indices, transform=None, domain_map=None):
        self.base_dataset = dataset
        self.indices = indices
        self.transform = transform
        self.domain_map = domain_map  # dict: dataset_name -> int

    def __getitem__(self, idx):
        sample = self.base_dataset[self.indices[idx]]
        if self.transform:
            sample = self.transform(sample)

        # ç”Ÿæˆ domain_label
        if self.domain_map is not None and 'metadata' in sample:
            meta = sample['metadata']
            dataset_name = meta.get('data', None)  # éœ€è¦ä¿è¯ SlidingWindowDataset æœ‰è¿™ä¸ªå­—æ®µ
            if dataset_name is not None and dataset_name in self.domain_map:
                sample['domain_label'] = torch.tensor(self.domain_map[dataset_name], dtype=torch.long)
            else:
                sample['domain_label'] = torch.tensor(-1, dtype=torch.long)  # æœªçŸ¥åŸŸ

        return sample

    def __len__(self):
        return len(self.indices)


def audit_actions(dataloader, name="Train", enabled=True):
    if not enabled:
        return set(), Counter(), defaultdict(set)

    action_counts = Counter()
    subjects_per_action = defaultdict(set)
    total_windows = 0

    for batch in dataloader:
        meta = batch['metadata']
        # å…¼å®¹ä¸¤ç§ç»“æ„ï¼šmeta æ˜¯ list(dict) æˆ– dicté‡ŒåŒ…å«æ‰¹å†…åˆ—è¡¨
        if isinstance(meta, list):
            for m in meta:
                act = str(m.get('activity_name', 'UNKNOWN'))
                sid = str(m.get('subject_id', 'UNKNOWN'))
                action_counts[act] += 1
                subjects_per_action[act].add(sid)
                total_windows += 1
        elif isinstance(meta, dict):
            acts = meta.get('activity_name')
            sids = meta.get('subject_id')
            # æŠŠæ‰¹å†…æ¯ä¸ªæ ·æœ¬å±•å¼€
            if isinstance(acts, (list, tuple)) and isinstance(sids, (list, tuple)):
                for act, sid in zip(acts, sids):
                    act = str(act)
                    sid = str(sid)
                    action_counts[act] += 1
                    subjects_per_action[act].add(sid)
                    total_windows += 1
            else:
                act = str(acts)
                sid = str(sids)
                action_counts[act] += 1
                subjects_per_action[act].add(sid)
                total_windows += 1
        else:
            raise TypeError(f"Unexpected metadata type: {type(meta)}")

    actions = set(action_counts.keys())
    print(f"[{name}] windows={total_windows}, unique_actions={len(actions)}")
    # åˆ—å‡ºæ¯ä¸ªåŠ¨ä½œçš„çª—å£æ•°ä¸å—è¯•è€…æ•°
    for act in sorted(actions):
        print(f"  - {act}: windows={action_counts[act]}, subjects={len(subjects_per_action[act])}")

    return actions, action_counts, subjects_per_action


def create_dataloaders(config, modality):
    """
    åˆ›å»ºè‡ªç›‘ç£å­¦ä¹ (SSL)è®­ç»ƒçš„DataLoaderã€‚

    Args:
        config (dict): å…¨å±€é…ç½®å­—å…¸
        modality (str): æ¨¡æ€ç±»å‹ ("emg" æˆ– "imu")

    Returns:
        dict: {'train': train_loader, 'val': val_loader}
    """

    data_cfg = config['data']
    train_cfg = config['train']

    # ===== Step 1: åŠ è½½åŸºç¡€æ•°æ®é›† =====
    logger.info("ğŸ“‚ Loading base datasets...")

    knee_pad = KneePAD(root_dir=data_cfg['knee_pad_root'])
    move_port = MovePort(root_dir=data_cfg['move_port_root'])
    base_datasets = [knee_pad, move_port]

    # ===== Step 2: åˆ›å»ºæ»‘åŠ¨çª—å£æ•°æ®é›† =====
    logger.info("ğŸ“Š Creating sliding window dataset...")
    full_dataset = SlidingWindowDataset(
        base_datasets=base_datasets,
        window_sec=data_cfg['window_sec'],
        step_sec=data_cfg['step_sec'],
        target_sr=data_cfg['target_sr'],
        cache_dir=data_cfg['cache_dir'],
        enable_filtering=data_cfg['enable_filtering'],
        config=config,
        num_jobs=data_cfg.get('num_jobs', 4),
        force_rebuild=data_cfg.get('force_rebuild', False),
        modality=modality
    )

    # ===== Step 3: ç¨€æœ‰åŠ¨ä½œè¿‡æ»¤ï¼ˆæ–¹æ³•1ï¼‰ =====
    min_subj_per_act = data_cfg.get('min_subjects_per_action', None)
    rare_actions = set()
    if min_subj_per_act is not None:
        from collections import defaultdict
        # ç»Ÿè®¡æ¯ä¸ªåŠ¨ä½œçš„å—è¯•è€…é›†åˆ
        subjects_per_action = defaultdict(set)
        for win in full_dataset.window_index:
            act = str(win['metadata']['activity_name'])
            sid = str(win['metadata']['subject_id'])
            subjects_per_action[act].add(sid)

        rare_actions = {act for act, subj_set in subjects_per_action.items()
                        if len(subj_set) < min_subj_per_act}
        if rare_actions:
            logger.info(f"[FILTER] Removing rare actions with subjects < {min_subj_per_act}: {sorted(rare_actions)}")
        else:
            logger.info("[FILTER] No rare actions found.")

    # ===== Step 4: æ•°æ®é›†åˆ’åˆ† =====
    if data_cfg.get('subject_split_path') and os.path.exists(data_cfg['subject_split_path']):
        split = torch.load(data_cfg['subject_split_path'])
        train_subjects = set(split['train_subjects'])
        val_subjects = set(split['val_subjects'])
        logger.info("ğŸ“‚ Loaded subject split from file, ensuring consistent train/val subjects.")

    else:
        seed = config.get("seed", 42)
        torch.manual_seed(seed)

        # è·å–æ‰€æœ‰ unique subject_id
        all_subjects = list({win['metadata']['subject_id'] for win in full_dataset.window_index})
        logger.info(f"ğŸ“Š Total unique subjects: {len(all_subjects)}")

        # éšæœºæ‰“ä¹±å—è¯•è€…é¡ºåº
        if isinstance(all_subjects[0], str):
            import random
            random.Random(seed).shuffle(all_subjects)
        else:
            all_subjects = torch.tensor(all_subjects)
            all_subjects = all_subjects[torch.randperm(len(all_subjects))].tolist()

        train_subject_count = int(0.8 * len(all_subjects))
        train_subjects = set(all_subjects[:train_subject_count])
        val_subjects = set(all_subjects[train_subject_count:])

        # ä¿å­˜åˆ’åˆ†
        torch.save({
            'train_subjects': list(train_subjects),
            'val_subjects': list(val_subjects)
        }, data_cfg.get('subject_split_path'))

    # æ ¹æ® subject è¿‡æ»¤ + ç¨€æœ‰åŠ¨ä½œè¿‡æ»¤çª—å£ç´¢å¼•
    train_indices = [
        i for i, win in enumerate(full_dataset.window_index)
        if win['metadata']['subject_id'] in train_subjects
           and (str(win['metadata']['activity_name']) not in rare_actions)
    ]
    val_indices = [
        i for i, win in enumerate(full_dataset.window_index)
        if win['metadata']['subject_id'] in val_subjects
           and (str(win['metadata']['activity_name']) not in rare_actions)
    ]

    logger.info(f"ğŸ“Š Train samples(after filter): {len(train_indices)}, Val samples(after filter): {len(val_indices)}")

    # ===== è°ƒè¯•æ—¥å¿— - æ£€æŸ¥äº¤å‰å—è¯•è€… =====
    overlap_subjects = train_subjects & val_subjects
    if overlap_subjects:
        logger.warning(
            f"âš ï¸ æ•°æ®æ³„éœ²é£é™©: {len(overlap_subjects)}ä¸ªå—è¯•è€…åœ¨ Train å’Œ Val é›†éƒ½æœ‰å‡ºç°: {overlap_subjects}")
    else:
        logger.info("âœ… æ— å—è¯•è€…äº¤å‰ï¼Œæ•°æ®åˆ’åˆ†å®‰å…¨ã€‚")

    # ===== Step 4: åˆ›å»ºSSLæ•°æ®å¢å¼º Transform =====
    ssl_transform = create_ssl_transforms(
        mask_ratio=train_cfg['mask_ratio'],
        num_views=train_cfg['num_views'],
        num_chunks=train_cfg['num_chunks'],
        modality=modality
    )

    # ===== Step 5: æ„é€ å­é›†æ•°æ®é›† =====

    # å®šä¹‰ domain æ˜ å°„è§„åˆ™ï¼ˆè¿™é‡Œç”¨æ•°æ®é›†æ¥æºï¼‰
    domain_map = {"KneePAD": 0, "MovePort": 1}

    train_ds = TransformSubsetDataset(full_dataset, train_indices, ssl_transform, domain_map=domain_map)
    val_ds = TransformSubsetDataset(full_dataset, val_indices, ssl_transform, domain_map=domain_map)

    # ===== Step 6: DataLoader æ€§èƒ½ä¼˜åŒ–å‚æ•° =====
    worker_count = min(data_cfg.get('num_workers', 4), os.cpu_count() or 1)
    prefetch_factor = data_cfg.get('prefetch_factor', 4)

    logger.info(f"ğŸ”§ DataLoader workers: {worker_count}, prefetch_factor: {prefetch_factor}")

    # ===== Step 7: åˆ›å»º DataLoader =====
    train_loader = DataLoader(
        train_ds,
        batch_size=data_cfg['batch_size'],
        shuffle=True,
        num_workers=worker_count,
        pin_memory=False,
        persistent_workers=True,
        prefetch_factor=prefetch_factor,
        drop_last=True
    )

    val_loader = DataLoader(
        val_ds,
        batch_size=data_cfg['batch_size'],
        shuffle=False,
        num_workers=worker_count,
        pin_memory=False,
        persistent_workers=True,
        prefetch_factor=prefetch_factor,
        drop_last=False
    )

    # # ===== å¯é€‰å®¡è®¡ =====
    # enable_audit = data_cfg.get('enable_audit', True)

    # if enable_audit:
    #     train_actions, train_counts, train_subj_per_act = audit_actions(
    #         train_loader, "Train", enabled=True)
    #     val_actions, val_counts, val_subj_per_act = audit_actions(
    #         val_loader, "Val", enabled=True)

    #     global_actions = train_actions | val_actions
    #     missing_in_train = global_actions - train_actions
    #     missing_in_val = global_actions - val_actions

    #     print("Global actions:", sorted(global_actions))
    #     print("Missing in Train:", sorted(missing_in_train))
    #     print("Missing in Val:", sorted(missing_in_val))
    # else:
    #     logger.info("ğŸš« Action audit disabled by config.")

    logger.info(f"âœ… DataLoaders created - Train batches: {len(train_loader)}, Val batches: {len(val_loader)}")
    logger.info(
        f"ğŸ”§ SSL Transform config - mask_ratio: {train_cfg['mask_ratio']}, num_views: {train_cfg['num_views']}, num_chunks: {train_cfg['num_chunks']}")

    return {'train': train_loader, 'val': val_loader}